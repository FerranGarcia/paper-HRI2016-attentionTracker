% This is "sig-alternate.tex" V2.0 May 2012
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate}
\graphicspath{{figs/}}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Alternate {\ttlit ACM} SIG Proceedings Paper in LaTeX
Format}
\author{Fernando Garcia\qquad Séverin Lemaignan \qquad Pierre Dillenbourg\\Computer-Human Interaction in Learning and Instruction Laboratory (CHILI)\\École Polytechnique Fédérale de Lausanne (EPFL)}

%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

%\numberofauthors{4} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
%\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
%\alignauthor
%Ben Trovato\titlenote{Dr.~Trovato insisted his name be first.}\\
%       \affaddr{Institute for Clarity in Documentation}\\
%       \affaddr{1932 Wallamaloo Lane}\\
%       \affaddr{Wallamaloo, New Zealand}\\
%       \email{trovato@corporation.com}
%% 2nd. author
%\alignauthor
%G.K.M. Tobin\titlenote{The secretary disavows
%any knowledge of this author's actions.}\\
%       \affaddr{Institute for Clarity in Documentation}\\
%       \affaddr{P.O. Box 1212}\\
%       \affaddr{Dublin, Ohio 43017-6221}\\
%       \email{webmaster@marysville-ohio.com}
%% 3rd. author
%\alignauthor Lars Th{\o}rv{\"a}ld\titlenote{This author is the
%one who did all the really hard work.}\\
%       \affaddr{The Th{\o}rv{\"a}ld Group}\\
%       \affaddr{1 Th{\o}rv{\"a}ld Circle}\\
%       \affaddr{Hekla, Iceland}\\
%       \email{larst@affiliation.org}
%\and  % use '\and' if you need 'another row' of author names
%% 4th. author
%\alignauthor Lawrence P. Leipuner\\
%       \affaddr{Brookhaven Laboratories}\\
%       \affaddr{Brookhaven National Lab}\\
%       \affaddr{P.O. Box 5000}\\
%       \email{lleipuner@researchlabs.org}
%}

\maketitle
\begin{abstract}
In long term interactions between a human and a robot, it is necessary that the agent exhibits some sort of social intelligence to be able to engage the human providing the appropriate response to specific face-to-face situations. In a educational context, this feature becomes a must due to the fact that engagement plays a very important role in the learning process. Therefore, maximizing the interaction time at a high engagement level may provide learning benefits to the children. In this work, a real-time attention tracker based on the head pose estimation using an RGB camera is proposed to be able to identify faults during the interaction and eventually detect low engagement level situations. In this paper we want to show a practical approach to evaluate the engagement that arises from the interaction reporting several experiences in the field, with the robot Nao.
\end{abstract}

%% A category with the (minimum) three required fields
%\category{H.4}{Information Systems Applications}{Miscellaneous}
%%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]
%
%\terms{Theory}
%
%\keywords{ACM proceedings, \LaTeX, text tagging}

\section{Introduction}
The use of robots in the learning activity presents an opportunity for the children
to interact with an embodied, physical agent as part of the learning experience. Furthermore,
the use of robots in handwriting education comes with the potential to engage the child in
meta-cognition through the learning by teaching paradigm, wherein a student takes the role of
a teacher and experiences stronger educational benefits as a result (such as in \cite{Palinscar1984}).

It becomes a useful to capture the significant passive information provided by the
user during the interaction and adapt the situation to it.

The present studies has been developed in the context of the CoWriter project \footnote{The primary repository is \url{https://github.com/chili-epfl/cowriter_letter_learning}.}.

\section{Approach}

In our approach we use the open-source library dlib\footnote{http://dlib.net} \cite{dlib09}, to detect and extract the skeleton of the subject's face and then, use the obtained landmarkers to calculate the head pose orientation estimation. In this way we can estimate where the subject was looking at. Compared to intrusive hardware like head mounted displays or glasses but also images acquired from an RGB-D cameras for instance, this approach has a clear advantage of simplicity but also becomes accurate enough for general purposes in face-to-face interactions. In addition is able to handle different number and positions of participants in the scenario.

The first step is to estimate the head pose using the pin-hole model shown in equation \ref{eq:pinHole}. As we mention before dlib library facilitates the task by providing the 2D points \textit{m'} that composes the face silhouette. However, it is enough with obtaining eight spread points, such as right and left eyes, right and left tragions, sellion or nasal depression zone, pronasale or center of the nose, stomion or center of the mouth and menton.

Likewise, the 3D face model is needed. The proposed head pose estimation method would work best with a 3D face model of the query subject itself. When a 3D face model of the subject is not available, as in many practical situations, a 3D face model of a subject can be used. This is an estimation of the same anthropometric points \textit{M'} (for North American Caucasians) in the 3D space \cite{farkas1994anthropometry}. Moreover, the intrinsic camera parameters are necessary to be able to compose the camera matrix \textit{A}. The components are expanded in equation \ref{eq:pinHole2}.	

\begin{equation}
s  \; m' = A [R|t] M'
\label{eq:pinHole}
\end{equation}
\begin{equation}
s
\begin{bmatrix}
x \\
y \\
1
\end{bmatrix}
=
\begin{bmatrix}
f_x & 0 & c_x  \\
0 & f_y & c_y  \\
0 & 0 & 1 
\end{bmatrix}
\begin{bmatrix}
r_{11} & r_{12} & r_{13} & t_x  \\
r_{21} & r_{22} & r_{23} & t_y  \\
r_{31} & r_{32} & r_{33} & t_z  
\end{bmatrix}
\begin{bmatrix}
X \\
Y \\
Z \\
1
\end{bmatrix}
\label{eq:pinHole2}
\end{equation}

Finally, the matrix of extrinsic parameters [R|t] is obtained by joining the rotation matrix R (calculated from the rotation vector) and translation vector \textit{t}. The result is a matrix projection \textbf{P} which maps a point in the 3D space onto a point in the 2D image place solving the equation \ref{eq:p}.

\begin{equation}
s  \; m' = P \; M'
\label{eq:p}
\end{equation}

The second step is to use the matrix [R|t] to set the correct translation and orientation of the \textit{tf} frame which represents the face in the 3D space. In order to achieve that it is necessary first, to set the origin defined by the vector \textit{t}. Second, the matrix \textit{R} needs to be reformulated into a quaternion as shown in equation \ref{eq:quat} due to the smoothly interpolation between them.

\begin{equation}
\begin{bmatrix}
qw \\
qx \\
qy \\
qz
\end{bmatrix}
=
\begin{bmatrix}
\sqrt{1 + r_{11} + r_{22} + r_{33}} /2 \\
(r_{32} - r_{23})/( 4 \cdot qw) \\
(r_{13} - r_{31})/( 4 \cdot qw) \\
(r_{21} - r_{12})/( 4 \cdot qw)
\end{bmatrix}
\label{eq:quat}
\end{equation}
\\
However, it is necessary to consider the cases where the division is performed by 0 (in 180\degree rotation about the y-axis for instance) or when \textit{qw} becomes close to zero.

The third step is to generate a representation of the field of view which origin and orientation are the ones defined by the face detection \textit{tf} frame. 

Defining accurately the human field of view in the implementation is a must. Holmqvist in \cite{holmqvist2011eye} specifies that the visual human range is $ \pm  40\degree $ in the horizontal and $ \pm 25\degree $ in the vertical. In \cite{walker1980clinical} Spector provides a more detailed specification for each eye splitting the vertical range into $ 60\degree $ the upper region and $ 75\degree $ the lower one. Moreover, the horizontal range gets separated in $ 60\degree $ inwards (towards the nose) and $ 95\degree $ outwards. In the present implementation, the first approach explained has been chosen representing the field of view using a cone with such dimensions.

Assuming that the \textit{tf} frames representing the robotic agent, the two tablets, the experimenter and the observers are static, need to be monitored. This task consists on evaluating the intersection between the \textit{tf} frames and the field of view: If one focus of attention is within the region defined by the field of view, we assume the subject is looking at it. In order to solve such operation mathematically it is necessary to compute the transformation matrix between the coordinate frames to be able to locate the monitored frames \textit{A} in the subject's coordinate system \textit{B} (see equation \ref{eq:transform}).

\begin{equation}
v' = B(A^{-1})v
\label{eq:transform}
\end{equation}

where \textit{v} is a point in \textit{A} that becomes v' in coordinate system \textit{B}. However, in practice there are, at least, three different \textit{tf} frames involved in the transformation (subject-camera, camera-base, base-focus). Following the transformation result, the distance of the monitored frame with respect to the x-axis face \textit{tf} frame, $ d_{x_{axis}} $, representing the subject's face can be computed as simple as in equation \ref{eq:pitagoras}. Always considering the x-axis as the one defining the face's orientation and thus field of view's main axis.

\begin{equation}
d_{x_{axis}} = \sqrt{t_y^2 + t_z^2}
\label{eq:pitagoras}
\end{equation}

It is necessary to compare the distance acquired $ d_{x_{axis}} $ with the radius of the field of view at the target \textit{tf} frame x-coordinate $ r_{fov} $ as shown in equation \ref{eq:fovtf}.

\begin{equation}
r_{fov} = tan\left(\frac{fov}{2}\right) \cdot t_x
\label{eq:fovtf}
\end{equation}

where \textit{fov} is the aperture angle of the field of view. Finally, if $ d_{x_{axis}}<r_{fov} $, the target is within the subject's field of view.

% Plot to clarify this??


\subsection{Experimental Set-up}

The target of this study were 6 children with ages compressed between 5 and 6 years old. The experiments were developed individually and consisted on a writing activity by turns and one story telling performed by the robot at a given time of the interaction.

Figure~\ref{realSetup} illustrates our general experimental setup: a
face-to-face child-robot interaction with an (autonomous) Aldebran's {\sc nao}
robot.

\begin{figure}
    \centering
    \includegraphics[width=1\columnwidth]{realSetup}
    \caption{\small Our experimental setup: face-to-face interaction with a {\sc
            nao} robot.  The robot writes on the tactile tablet, the child then
            corrects the robot by directly overwriting its letters on the tablet
            with a stylus. An adult (either a therapist or an experimenter,
            depending on the studies), remains next to the child to guide the work. 
            A second tablet allows to choose words and the camera captures the subject's face.}
    \label{realSetup}
\end{figure}

A tactile tablet (with a custom application) is used for both the robot and the
child to write: during a typical round, the child requests the robot to write
something (a single letter, a number or a full word), and push the tablet
towards the robot, the robot writes on the tablet by gesturing the writing (but
without actually physically touching the tablet), the child then pull back the
tablet, corrects the robot's attempt by writing him/herself on top or next to
the robot's writing (see Figure~\ref{fig:diego}), and ``send'' his/her
demonstration to the robot by pressing a small button on the tablet. The robot
``learns'' from this demonstration and tries again.

Since the child is assumed to take on the role of the teacher, we had to ensure
(s)he would be able to manage by him/herself the turn-taking and the overall
progression of the activity (moving to the next letter or word). In our design,
the turn-taking relies on the robot prompting for feedback once it is done with
its writing (simple sentences like ``What do you think?''), and pressing on a
small robot icon on the tablet once the child has finished correcting. In our
experiments, both were easy to grasp for children.

Implementing such a system raises several challenges that are discussed in detail in \cite{Hood:2015}.

An RGB camera has been used to acquire images of 640x480 pixels at 25 fps. The camera was located at 5 cm from the center of Nao's feet and its holder was tied to a wood base for stability purposes. Thus, the camera's objective was 9 cm above and $ 40\degree $ inclination with respect to the surface of the table. 
The subjects were typically located 50 cm away from the robot with the first tablet in front and the second one 30 cm to the left of the first one. In the same way, the experimenter was located around 60 cm to the left of the subject. Finally, two observers were located far away from the interaction field to manually assess the state of the interaction.

Figure \ref{drawSetup} allows us to intuitively identify several focuses of attention along the interaction such as the two tablets, the robot, the experimenter and the observers located in the diagonal.

\begin{figure}
    \centering
    \includegraphics[width=0.7\columnwidth]{drawSetup}
    \caption{\small Graphical representation of he set-up and the possible children focuses of attention in the scene, in red.}
    \label{drawSetup}
\end{figure}


\section{Results}

In this work, head orientation is used to predict a person's focus of attention in face-to-face interactions. Therefore, we assume that head pose is a reliable marker of the attention focus during a social interaction, in both cases human-human, human-robot. Since we estimate where the subject is looking at based on its head position, several important questions arise:

\begin{enumerate}
\item Which percentage can we achieve during a real case scenario in terms of head pose estimation tracking?  
\item How accurate can we predict where the subject is looking at based only on the head pose estimation?
\item How can we evaluate the performance of the children based on the focus of attention?
\end{enumerate}

% Stats from the paper
% Stats from me
% Real vs captured plot
% Real vs expected plot

\subsection{From Face Detection to Focus of Attention}

According to \cite{stiefelhagen2002tracking} the head orientation's contribution in overall gaze direction is 68.9\% showing agreement between both measurements. In addition, it shows that the head orientation alone can get an average accuracy of 88.7\% detecting the focus of attention in a meeting application scenario using a head-mounted display with eye and head tracking.

Our results for a peer-to-peer work scenarios using an approach based on RGB camera are summarized in table \ref{tab:results} where the \textit{expected} is the focus of attention that given the state of the system, it is likely to attract the subject's gaze. In the same manner, the \textit{captured} is the focus of attention our approach provides and the \textit{real}, the ground truth of the subject's gaze direction based on manual annotations of two evaluators.

\begin{table}[h!]
	\centering
	\caption{Tracking accuracy in face-to-face scenario}
	\begin{tabular}{|c|c|c|c|c|c|c|c|} \hline
	\small Subject & 1 & 2 & 3 & 4 & 5 & 6 & Avg\\ \hline
	\small Track(\%) & \small78.7 & \small81.1 & \small73.7 & \small75.6 & \small77.1 & \small68.7 & \small75.8 \\ \hline
	\begin{tabular}{@{}c@{}}\small Cosine \\ \small affinity \end{tabular} & \small0.58 & \small & \small & \small & \small & \small & \small \\ \hline
	\begin{tabular}{@{}c@{}}\small Real vs \\ \small expected(\%)\end{tabular} & \small87.3 & \small  & \small & \small & \small & \small & \small \\ \hline
	\begin{tabular}{@{}c@{}}\small Real vs \\ \small captured(\%)\end{tabular} & \small59.4 & \small & \small & \small & \small & \small & \small \\ \hline
	\end{tabular}
	\label{tab:results}
\end{table}

The results has shown that the percentage of accuracy in the tracking is not only conditioned by the subject's amount of movement, but also by the speed of the same movements: Restless children externalize greater levels of movement decreasing the tracker accuracy. Additionally, several recurrent computer vision challenges such as the lighting conditions, occlusions and the face angle with respect to the camera are factors that contribute to reduce the accuracy.

\subsection{Subject's Focus of Attention Analysis}


% This two plots are provisional. We expect to improve them with images and boxes
\begin{figure*}
    \centering
    \includegraphics[width=2\columnwidth]{realExpected}
    \caption{\small In blue, real focus accoding to two observer assessment. In green, the expected focus based on the system state and in cyan, the overlapping between them.}
    \label{fig:realExpected}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=2\columnwidth]{realCaptured}
    \caption{\small In blue, real focus accoding to two observer assessment. In green, the captured focus using our aproach and in cyan, the overlapping between them.}
    \label{fig:realCaptured}
\end{figure*}
% % % % % % % % % % % % % % % % % % % %

In several occasions during the interaction, none of the focuses of attention were within the field of view due to the time transition from one focus to another. In order to deal with these few cases we assume that the minimum distance between the height of the field of view (cone) and the targets corresponds to the most likely focus of attention.

However, we also want to distinguish such cases when the subject is directly not looking where the defined targets are located, but towards a region far from the interaction field. These cases are not common but they can be easily identified by setting a horizontal threshold to the field of view.

Several results can extracted from figure \ref{fig:realExpected}. First, the matching curve shows the overlapping between the expected and the real focuses of attention. As we can see there is a fluctuation at the beginning of the interaction suggesting a novelty effect. It suggests that the adaptation time is around the first 2 minutes in average. 

Second, once the turn-taking is established, a pattern becomes visible during the subject's feedback, when a word is being written by the children: Such pattern shows a high intermittent gaze frequency from the tablet to the selection. It shows that the subject is using the word model in the selection tablet to provide a better answer to the robot. But not only that, the subjects also decrease the response time after each shift as well as the gaze frequency towards the model suggesting and improvement over time. Moreover, when a new word is selected for playing, the same behavior is replicated.

A third trend that can be observed is the continuous 'look for approval' situation where the subjects look at the experimenter in order to provide feedback about the robot's response before providing their own one. Such transition tablet-facilitator becomes more frequent in several cases, for instance: An unexpected situation is faced or the facilitator make a suggestion. 

Finally, the subjects set their attention more often to the tablet displaying the generated hand-writing during the robot feedback rather than to the robot itself. 

\section{Conclusions}

%ACKNOWLEDGMENTS are optional
\section*{Acknowledgments}
This research was partially supported by the Funda\c{c}\~{a}o para a Ci\^{e}ncia
e a Tecnologia (FCT) with reference UID/CEC/ 50021/2013, and by the Swiss
National Science Foundation through the National Centre of Competence in
Research Robotics.

\bibliographystyle{abbrv}
\bibliography{sigproc}
\end{document}
